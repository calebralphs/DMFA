{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import math\n",
    "import pprint\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "The purpose of this cell is to select all of the data from the master excel sheet that is difficult to handle and output this data to a new excel sheet to be further altered and added to. Once ran once, this cell is pretty much rendered useless because it would overwrite any additions you make on the target output excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2d9e11564161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Importing master excel document and extracting relevant data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input_Final_LIBSRATE.xlsx'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Input'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mauto_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauto_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m82\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdesired_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Parameters:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Population (low)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CpC (baseline)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.6'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.7'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.9'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.10'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.11'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.12'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.13'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S1.14'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing master excel document and extracting relevant data\n",
    "data_raw = pd.read_excel('Input_Final_LIBSRATE.xlsx', 'Input', header = 4)\n",
    "auto_data = data_raw.drop(list(range(0, 9)))\n",
    "auto_data.drop(list(auto_data.columns)[82:], axis = 1)\n",
    "desired_columns = ['Parameters:', 'Population (low)', 'CpC (baseline)', 'S1.2', 'S1.3', 'S1.4', 'S1.5', 'S1.6', 'S1.7', 'S1.8', 'S1.9', 'S1.10', 'S1.11', 'S1.12', 'S1.13', 'S1.14']\n",
    "auto_data = pd.concat([auto_data[col] for col in auto_data.columns if col in desired_columns], axis = 1)\n",
    "auto_data.columns = ['Year', 'Population', 'CpC', 'Closures', 'Bumpers', 'Engine Blocks', 'Heat Exchangers', 'Cylinder Heads', 'Suspension', 'Steering', 'Wheels', 'Transmission/Driveline', 'Brake Componenets', 'Other Engine', 'Body', 'Other Components']\n",
    "auto_data.set_index('Year')\n",
    "writer = pd.ExcelWriter('LIBS Rate ipynb Output Data.xlsx')\n",
    "auto_data.to_excel(writer, 'Auto Data', index = False, index_label = 'Year')\n",
    "\n",
    "# Importing alloy information\n",
    "alloys_raw = sio.loadmat('alloys_info.mat')\n",
    "\n",
    "# Parsing .MAT raw data into hierarchical array representation\n",
    "# [alloy, type, [nominal[], min[], max[]]]\n",
    "alloys_info = []\n",
    "for alloy in alloys_raw['alloys_info'][0]:\n",
    "    alloys_info.append([alloy[0][0][0][0], alloy[3][0], 'Nominal', *list(alloy[1][0])])\n",
    "    alloys_info.append([alloy[0][0][0][0], alloy[3][0], 'Minimum', *list(alloy[2][1])])\n",
    "    alloys_info.append([alloy[0][0][0][0], alloy[3][0], 'Maximum', *list(alloy[2][0])])\n",
    "alloys_info = pd.DataFrame(alloys_info)\n",
    "alloys_info.columns = ['Alloy', 'Type', 'Level', 'Si', 'Fe', 'Cu', 'Mn', 'Mg', 'Cr', 'Ni', 'Zn', 'Ti']\n",
    "alloys_info_table = pd.DataFrame(alloys_info)\n",
    "#alloys_info_table = pd.pivot_table(alloys_info, \n",
    "#                       index = ['Alloy', 'Type', 'Level'],\n",
    "#                       values = ['Level', 'Si', 'Fe', 'Cu', 'Mn', 'Mg', 'Cr', 'Ni', 'Zn', 'Ti'])\n",
    "alloys_info_table.to_excel(writer, 'Alloy Data', index = False)\n",
    "alloys_info.set_index('Alloy', drop = True)#['Alloy', 'Type', 'Level'], drop = True)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Scenario Conditions, Parameters, and Wrangle Data\n",
    "\n",
    "TODO:\n",
    "- Create function to input parameters for scenarios\n",
    "- Add parameters for starting year, and whatever criterion will be dynamic between this model and the regional ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### SCENARIO CONDITIONS #####\n",
    "# Population: 1 = baseline;\n",
    "population_scen = 1 # PQ\n",
    "\n",
    "# Cars per capita: 1 = baseline\n",
    "cars_per_capita_scen = 1 # CpCQ\n",
    "\n",
    "# Dismantling before ELVyear: 0 = none, 1 = low, 2 = high, 3 = all\n",
    "ELV_year = 1990 # ELVyear\n",
    "dismant_pre_ELV_scen = 0 # ELVQ1\n",
    "\n",
    "# Dismantling from ELVyear: 0 = none, 1 = low, 2 = high, 3 = all\n",
    "dismant_post_ELV_scen = 1 # ELVQ2\n",
    "\n",
    "# Alloy sorting scenario: 0 = none, 2 = laser sorting\n",
    "sorting_scen = 2 # ZQ\n",
    "\n",
    "# LIBS implmentation rate: 0 = none, 1 = low, 2 = medium, 3 = immediate, 100%\n",
    "LIBS_rate_scen = 1 #LIBSrate\n",
    "\n",
    "# Lifetime scenario: 2 = baseline, 1 = low, 3 = high\n",
    "lifetime_scen = 2 #LTQ\n",
    "\n",
    "# Demagging: 1 = on, 2 = off\n",
    "demag_scen = 1 #DMGQ\n",
    "\n",
    "# Zorba scenario: 0 = current, 1 = 2% reduction\n",
    "zorba_export_scen = 0 # Zorba_export\n",
    "\n",
    "##### PARAMETERS #####\n",
    "# Number of alloy groups that are sorted, e.g. 1xxx, 3xxx\n",
    "num_sorted_alloy_groups = 8 # NAG\n",
    "\n",
    "# This parameter reduces the amount of scrap that can be intelligently sorted as empirical data \n",
    "# has been collected that indicates that approximately 25% of scrap partiuculates are under 1 inch\n",
    "sizelimited = 0.25\n",
    "\n",
    "##### Variables #####\n",
    "\n",
    "# importing all sheets from excel sheet: 'LIBS Rate Input Data.xlsx'\n",
    "data = pd.read_excel('LIBS Rate Input Data.xlsx', sheet_name = None)\n",
    "old_scrap = data['Old Scrap']                                                                               # other_old_scrap\n",
    "old_scrap_comp = data['Old Alloy Compositions']                                                             # other_comp\n",
    "laser_sorting_alloy_groups = data['Laser Sorting Alloy Groups']                                             # alloy_names\n",
    "#####################################################################\n",
    "\n",
    "alloy_data = data['Alloy Data'].reset_index(drop = True)\n",
    "#alloy_data = pd.MultiIndex.from_arrays(data['Alloy Data'].values)\n",
    "\n",
    "#####################################################################\n",
    "auto_data = data['Auto Data']\n",
    "scenario_data = data['Scenarios']\n",
    "group_comp_by_alloy = data['Group Composition by Alloy']                                                    # AR_Ga\n",
    "group_comp_by_alloy.index = group_comp_by_alloy['Component']\n",
    "group_comp_by_alloy = group_comp_by_alloy.drop(columns = ['Component'])\n",
    "hand_sort = data['Hand Sorting']                                                                            # ZR_RaZ(:,:,1)       \n",
    "laser_sort = data['Laser Sorting']                                                                          # ZR_RaZ(:,:,2)\n",
    "primary_metal_comp = data['Primary Metal Composition']                                                      # CP_Ee\n",
    "element_yields = data['Element Yields']\n",
    "component_yields = data['Componenent Yields']\n",
    "opt_costs = data['Optimization Costs']\n",
    "manufacture_scrap_recipe = data['Manufacturing Scrap Recipe'].transpose()                                   # MSR_Gr\n",
    "\n",
    "# creating variables for names of headers\n",
    "alloy_names = list(old_scrap_comp.index)                                                                    # AlloyNames\n",
    "element_names = list(data['Old Alloy Compositions'].columns)                                                # ElementNames\n",
    "raw_material_names = list(data['Laser Sorting Alloy Groups'].index)                                         # RawMaterialNames\n",
    "comp_group_names = list(data['Auto Data'].columns)[3:]                                                      # GroupNames\n",
    "years = list(data['Auto Data']['Year'])                                                                     # Time\n",
    "\n",
    "# creating variables for number of columns for each feature\n",
    "num_raw_materials = len(raw_material_names)                                                                 # NR\n",
    "num_alloys = len(alloy_names)                                                                               # NA\n",
    "num_elements = len(element_names)                                                                           # NE\n",
    "num_component_groups = len(comp_group_names)                                                                # NG\n",
    "num_years = len(years)                                                                                      # NT\n",
    "num_segments = 1                                                                                            # NS\n",
    "num_cohorts = num_years # subject to change                                                                 # NC\n",
    "\n",
    "# creating misc variables\n",
    "# TODO: change values to be from the 'Scenarios' sheet of 'LIBS Rate Input Data'\n",
    "exp_life = 16                                                                                               # Mu\n",
    "std_dev = 3                                                                                                 # Sigma\n",
    "zorba_export_rate = [.45] * num_years\n",
    "collection_rate = [.98] * num_years                                                                         # CR_T\n",
    "shredder_yield = .95                                                                                        # SY\n",
    "secondary_alloys = True                                                                                     # SecondaryAlloysIndex\n",
    "primary_alloys = not secondary_alloys                                                                       # PrimaryAlloysIndex\n",
    "\n",
    "# importing data from within excel sheets\n",
    "population = auto_data['Population']                                                                        # P_T\n",
    "cars_per_capita = auto_data['CpC'] / 1000                                                                   # CpC_T\n",
    "segmentation = [1] * num_years # always 100% on spreadsheet (fraction of input)                             # SGM_Ts\n",
    "avg_Al_weight_per_comp = auto_data[list(auto_data.columns)[3:]]                                             # GW_TSG\n",
    "LIBS_rate = scenario_data['LIBS Rate' + str(LIBS_rate_scen)]                                                # LIBSrate\n",
    "ELV_rate_pre = data['ELV Scenario'].iloc[:, dismant_pre_ELV_scen]\n",
    "ELV_rate_post = data['ELV Scenario'].iloc[:, dismant_post_ELV_scen]\n",
    "ELV_year_index = scenario_data.Year[scenario_data.Year == ELV_year].index[0]\n",
    "ELV_by_comp = pd.DataFrame(np.concatenate((np.ones((ELV_year_index,1)).dot(ELV_rate_pre.to_frame().T),      # ELV_TG\n",
    "                                           np.ones((num_years - ELV_year_index,1)).\n",
    "                                           dot(ELV_rate_post.to_frame().T)), axis = 0),\n",
    "                           columns = list(auto_data.columns)[3:])                                                  \n",
    "sorting_rates = scenario_data[['Sorting' + str(sorting_scen) + ' Hand',                                     # Z_Tz\n",
    "                               'Sorting' + str(sorting_scen) + ' Laser']]     \n",
    "remelting_yields = element_yields.loc['Remelting Yields']                                                   # RY\n",
    "shredder_cont = element_yields.loc['Shredder Contamination']                                                # SC_e\n",
    "shredder_cont_dism = element_yields.loc['Shredder Contamination with Dismantled Parts']                     # SCD_e\n",
    "manufacture_yields = component_yields.loc['Manufacturing']                                                  # MY_G\n",
    "alloy_opt_costs = opt_costs.loc['Optimization Cost']                                                        # H_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X01 - Kg\n",
    "X01_TR = np.zeros((num_years, num_raw_materials))\n",
    "\n",
    "# X07a - Kg\n",
    "X07a_TE = np.zeros((num_years, num_elements))\n",
    "\n",
    "# X07b - Kg\n",
    "X07b_TE = np.zeros((num_years, num_elements))\n",
    "\n",
    "# X10 - Kg\n",
    "X10_TR = np.zeros((num_years, num_raw_materials))\n",
    "\n",
    "# X12 - Kg\n",
    "X12_TAE = np.zeros((num_years, num_alloys, num_elements))\n",
    "X12_TRA = np.zeros((num_years, num_raw_materials, num_alloys))\n",
    "X12_TR = np.zeros((num_years, num_raw_materials))\n",
    "X12_RAE = np.zeros((num_raw_materials, num_alloys, num_elements))\n",
    "\n",
    "# X20 - Kg\n",
    "X20_TAE = np.zeros((num_years, num_alloys, num_elements))\n",
    "\n",
    "# X23 - Kg\n",
    "X23_TAE = np.zeros((num_years, num_alloys, num_elements))\n",
    "X23_conc_TAE = np.zeros((num_years, num_alloys, num_elements))\n",
    "X23_TA = np.zeros((num_years, num_alloys))\n",
    "X23_TG = np.zeros((num_years, num_component_groups))\n",
    "X23_TGA = np.zeros((num_years, num_component_groups, num_alloys))\n",
    "X23_TGE = np.zeros((num_years, num_component_groups, num_elements))                                             # X23_TGe\n",
    "\n",
    "# X31 - Kg\n",
    "X301_TG = np.zeros((num_years, num_component_groups))\n",
    "X31_TRE = np.zeros((num_years, num_raw_materials, num_elements))\n",
    "X31_TAR = np.zeros((num_years, num_alloys, num_raw_materials))\n",
    "X301_TGA = np.zeros((num_years, num_component_groups, num_alloys))\n",
    "\n",
    "# X34 - Kg\n",
    "X34_TG = np.zeros((num_years, num_component_groups))\n",
    "\n",
    "# X40 - Export\n",
    "X40_TG = np.zeros((num_years, num_component_groups))\n",
    "X40_T = np.zeros((num_years, 1))\n",
    "\n",
    "# X45 - Cars, Kg\n",
    "X45_TS = np.zeros((num_years, num_segments)) # Cars\n",
    "X45_TG = np.zeros((num_years, num_component_groups))\n",
    "X45_T = np.zeros((num_years, 1))\n",
    "\n",
    "# X50 - Kg\n",
    "X50_TCG = np.zeros((num_years, num_cohorts, num_component_groups))\n",
    "\n",
    "# X56 - Kg\n",
    "X56_TCG = np.zeros((num_years, num_cohorts, num_component_groups))\n",
    "\n",
    "# X560 - Cars, Kg\n",
    "X560_T = np.zeros((num_years, 1))\n",
    "X560_TC = np.zeros((num_years, num_cohorts)) # Cars\n",
    "X560_TCG = np.zeros((num_years, num_cohorts, num_component_groups))\n",
    "X560_TCA = np.zeros((num_years, num_cohorts, num_alloys))\n",
    "X560_TCE = np.zeros((num_years, num_cohorts, num_elements))\n",
    "X560_TE = np.zeros((num_years, num_elements))\n",
    "\n",
    "# X67a - Kg\n",
    "X67a_TCG = np.zeros((num_years, num_cohorts, num_component_groups))\n",
    "X67a_TCGA = np.zeros((num_years, num_cohorts, num_component_groups, num_alloys))\n",
    "X67a_TCGE = np.zeros((num_years, num_cohorts, num_component_groups, num_elements))\n",
    "X67a_TGE = np.zeros((num_years, num_component_groups, num_elements))\n",
    "X67b_TA = np.zeros((num_years, num_alloys))\n",
    "\n",
    "# X67b - Kg\n",
    "X67b_TCA = np.zeros((num_years, num_cohorts, num_alloys))\n",
    "X67b_TAE = np.zeros((num_years, num_alloys, num_elements))\n",
    "X67b_TCAE = np.zeros((num_years, num_component_groups, num_alloys, num_elements))\n",
    "X67b_TCGE = np.zeros((num_years, num_cohorts, num_component_groups, num_elements))\n",
    "X67b_TAE = np.zeros((num_years, num_alloys, num_elements))\n",
    "X67b_T = np.zeros((num_years, 1))                                                                               # X67a_otherT\n",
    "X67b_TA = np.zeros((num_years, num_alloys))                                                                     # X67a_otherTA\n",
    "X67b_TAE_other = np.zeros((num_years, num_alloys, num_elements))                                                # X67b_otherTAE\n",
    "\n",
    "# X7a1 - Kg\n",
    "X7a1_TRE = np.zeros((num_years, num_raw_materials, num_elements))\n",
    "X7a1_TGE = np.zeros((num_years, num_component_groups, num_elements))\n",
    "\n",
    "# X7b1 - Kg\n",
    "X7b1_TRE = np.zeros((num_years, num_raw_materials, num_elements))\n",
    "X7b1b_TRE = np.zeros((num_years, num_raw_materials, num_elements))\n",
    "X7b1_TRE_nocont = np.zeros((num_years, num_raw_materials, num_elements))\n",
    "X7b1s_TRE = np.zeros((num_years, num_raw_materials, num_elements))\n",
    "\n",
    "# X81 - Kg\n",
    "X81_TRE = np.zeros((num_years, num_raw_materials, num_elements))\n",
    "\n",
    "# S (Stock) - Cars, Kg\n",
    "S_TCG = np.zeros((num_years, num_cohorts, num_component_groups))                                                # M5_TCG\n",
    "S_TC = np.zeros((num_years, num_cohorts)) # Cars                                                                # M5_TC\n",
    "S_T = np.zeros((num_years, 1)) # Cars                                                                              # M5_T\n",
    "S_TCA = np.zeros((num_years, num_cohorts, num_alloys))                                                          # M5_TCA\n",
    "S_TE = np.zeros((num_years, num_elements))                                                                      # M5_TE\n",
    "S_TCE = np.zeros((num_years, num_cohorts, num_elements))                                                        #M5_TCE\n",
    "\n",
    "# APR (Alloy Production Recipe) - Kg\n",
    "APR_adj_TRA = np.zeros((num_years, num_raw_materials, num_alloys))                                              # Yadj_TRA\n",
    "APR_adj_RAE = np.zeros((num_raw_materials, num_alloys, num_elements))                                           # Yadj_RAE\n",
    "APR_RA = np.zeros((num_raw_materials, num_alloys))                                                              # Y_RA\n",
    "APR_RAE = np.zeros((num_raw_materials, num_alloys, num_elements))                                               # Y_RAE\n",
    "APR_TRA = np.zeros((num_years, num_raw_materials, num_alloys))                                                  # Y_TRA\n",
    "\n",
    "# SRM (Suppy of Raw Materials) - Kg\n",
    "SRM_TRE = np.zeros((num_years, num_raw_materials, num_elements))                                                # U_TRE\n",
    "SRM_TR = np.zeros((num_years, num_raw_materials))                                                               # U_TR\n",
    "SRM_conc_TRE = np.zeros((num_years, num_raw_materials, num_elements))                                           # U_TRe\n",
    "for y in range(num_years):\n",
    "    SRM_conc_TRE[y][:num_elements + 1] = primary_metal_comp\n",
    "    SRM_TRE[y][:num_elements + 1] = [[math.inf] * len(primary_metal_comp.columns)] * len(primary_metal_comp.index)\n",
    "    SRM_TR[y] = np.sum(SRM_TRE[y], axis = 1)\n",
    "    if (demag_scen == 2):\n",
    "        SRM_conc_TRE[y][num_elements] = [0] * num_elements\n",
    "        SRM_TR[y][num_elements] = 0\n",
    "\n",
    "# D (amount of magnesium removed)\n",
    "D_TRA = np.zeros((num_years, num_raw_materials, num_alloys))                                                    # demagg_TRA\n",
    "D_TA = np.zeros((num_years, num_alloys))                                                                        # demagg_TA\n",
    "D_T = np.zeros(num_years)                                                                                       # demagg_T\n",
    "D_TAE = np.zeros((num_years, num_alloys, num_elements))                                                         # demagg_TAE\n",
    "\n",
    "# shred_contam (shredder contamination)\n",
    "shred_contam = np.zeros(num_years)                                                                              # shredder_contamination                                                                                      \n",
    "                                                                                                            \n",
    "##### Composition of Alloys #####\n",
    "# AC (alloy compositon)\n",
    "AC_upper_AE = np.zeros((num_alloys, num_elements))                                                              # CAU_Ae\n",
    "AC_lower_AE = np.zeros((num_alloys, num_elements))                                                              # CAL_Ae\n",
    "for i_a, a in enumerate(alloy_names):\n",
    "    a_df = alloy_data[alloy_data['Alloy'] == a] # equivalent to open_alloy(alloy_name, structure)\n",
    "    for i_e, e in enumerate(element_names[1:]):\n",
    "        if float(a_df[a_df['Level'] == 'Nominal'][e]) == 0:\n",
    "            AC_upper_AE[i_a][i_e] = float(a_df[a_df['Level'] == 'Maximum'][e]) / 100\n",
    "            AC_lower_AE[i_a][i_e] = 0\n",
    "        else:\n",
    "            AC_upper_AE[i_a][i_e] = float(a_df[a_df['Level'] == 'Nominal'][e]) / 100\n",
    "            AC_lower_AE[i_a][i_e] = float(a_df[a_df['Level'] == 'Nominal'][e]) / 100\n",
    "    AC_upper_AE[i_a][0] = 1 - sum(AC_lower_AE[i_a][1:])\n",
    "    AC_lower_AE[i_a][0] = 1 - sum(AC_upper_AE[i_a][1:])\n",
    "# set Al minimum content in 1070A to be .997 for some reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton Stock-Driven Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalDistTrunc(mu, sigma, time): # normal_distribution_trunc0\n",
    "    dist = np.zeros((time, 1))\n",
    "    for y in range(time):\n",
    "        dist[y] = norm.cdf(y + 1, mu, sigma) - norm.cdf(y, mu, sigma)\n",
    "    dist /= (1 - norm.cdf(0, mu, sigma))\n",
    "    return dist\n",
    "\n",
    "def stockDrivenModel(stock, dist): # stock_driven_model\n",
    "    time = len(stock)\n",
    "    stock_change = np.zeros((time, 1))\n",
    "    stock_change[0] = stock[0]\n",
    "    inp = np.zeros((time,1))\n",
    "    inp[0] = stock[0]\n",
    "    out = np.zeros((time, 1))\n",
    "    out_cohort = np.zeros((time, time))\n",
    "    stock_cohort = np.zeros((time, time))\n",
    "    stock_cohort[0][0] = inp[0]\n",
    "    \n",
    "    for ot in range(1, time): # output time\n",
    "        stock_change[ot] = stock[ot] - stock[ot - 1]\n",
    "        for it in range(ot): # input time\n",
    "            out_cohort[ot][it] = inp[it] * dist[ot - it - 1]\n",
    "        out[ot] = np.sum(out_cohort[ot])\n",
    "        inp[ot] = stock_change[ot] + out[ot]\n",
    "        for ct in range(ot + 1): # cohort time\n",
    "            #print(ot, sum(out_cohort[:(ot + 1)][ct]))\n",
    "            stock_cohort[ot][ct] = inp[ct] - sum(out_cohort[:ot + 1, ct])\n",
    "    return inp, out, out_cohort, stock_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stock Calculation\n",
    "S_T = np.multiply(cars_per_capita, population)\n",
    "\n",
    "prob_dist = normalDistTrunc(exp_life, std_dev, num_years)\n",
    "input_T, output_T, X560_TC, S_TC = stockDrivenModel(S_T, prob_dist)\n",
    "\n",
    "# X45 - Segmented Input\n",
    "X45_TS = input_T # 151 by 1, rather than 151 by 5 with columns 1-4 being zeros\n",
    "\n",
    "for t in range(num_years):\n",
    "    # input of groups - Kg (where GW refers to group weight)\n",
    "    X34_TG[t] = np.multiply(np.squeeze(avg_Al_weight_per_comp.loc[t]).transpose() * np.squeeze(X45_TS[t].transpose()), 1.8)\n",
    "    X45_TG[t] = np.divide(X34_TG[t], 1.8) # why 1.8\n",
    "    X40_TG[t] = X34_TG[t] - X45_TG[t]\n",
    "    # flow of material to manufacturing of component groups - Kg\n",
    "    X23_TG[t] = np.divide(X34_TG[t], manufacture_yields.transpose())\n",
    "    X301_TG[t] = X23_TG[t] - X34_TG[t]\n",
    "    # flow of material to alloys of component groups - Kg\n",
    "    X23_TGA[t] = np.diag(X23_TG[t]).dot(group_comp_by_alloy)\n",
    "    X31_TAR[t] = (manufacture_scrap_recipe.transpose().dot(np.squeeze(X301_TGA[t]))).transpose()\n",
    "\n",
    "X45_T = np.squeeze(np.sum(X45_TG, 1))\n",
    "X40_T = np.squeeze(np.sum(X40_TG, 1))\n",
    "\n",
    "\n",
    "for t in range(num_years):\n",
    "    for c in range(t + 1):\n",
    "        X560_TCG[t][c] = np.squeeze(X560_TC[t][c] * (segmentation[c] * np.squeeze(avg_Al_weight_per_comp.iloc[c])))\n",
    "        X560_TCA[t][c] = np.dot(group_comp_by_alloy.transpose(), np.squeeze(X560_TCG[t][c])).transpose()\n",
    "        X50_TCG[t][c] = X560_TCG[t][c] * (1 - collection_rate[t])\n",
    "        S_TCG[t][c] = np.dot(np.dot(np.squeeze(S_TC[t][c]), segmentation[c]), np.squeeze(avg_Al_weight_per_comp.iloc[c])) # incorrect\n",
    "        S_TCA[t][c] = np.dot(group_comp_by_alloy.transpose(), np.squeeze(S_TCG[t][c])).transpose() # incorrect because of S_TC\n",
    "        \n",
    "\n",
    "# X560_TCG (available scrap at EOL), X50_TCG (lost scrap), S_TCG (output and stocks as groups) - Kg\n",
    "X56_TCG = X560_TCG - X50_TCG\n",
    "X560_T = np.squeeze(np.sum(np.sum(X56_TCG, 2), 1))\n",
    "\n",
    "for t in range(num_years):\n",
    "    # X67a_TCG - dismantled parts\n",
    "    X67a_TCG[t] = np.dot(np.squeeze(X56_TCG[t]), np.diag(np.squeeze(ELV_by_comp.iloc[t]))) # sum difference: 2.2618 vs 2.2499 (e+10)\n",
    "    for c in range(t):\n",
    "        # X67b_TCA - shredded alloys\n",
    "        X67b_TCA[t][c] = np.dot(np.squeeze(X56_TCG[t][c] - X67a_TCG[t][c]).transpose(), group_comp_by_alloy) # sum difference: 1.4633 vs 1.4645 (e+11)\n",
    "        # X67a_TCGA - dismantled alloys per cohort and group\n",
    "        X67a_TCGA[t][c] = np.dot(np.diag(np.squeeze(X67a_TCG[t][c])), group_comp_by_alloy) # sum difference: 2.2618 vs 2.2499 (e+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22618453692.485382"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
